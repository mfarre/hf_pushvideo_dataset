{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a webdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import random\n",
    "import json\n",
    "import io\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Directories containing the videos and metadata\n",
    "# This script assumes that the name of the videos and the name of the metadata files are exactly the same, just changing the extension\n",
    "video_dir = Path('videos/')  # Folder with .mp4 files\n",
    "metadata_dir = Path('jsons_dataset_reformat/')  # Folder with .json files\n",
    "output_dir = Path('hf_dataset/')  # Output directory for dataset\n",
    "\n",
    "\n",
    "# Step 1: Find matching video-metadata pairs\n",
    "video_files = {f.stem: f for f in video_dir.glob('*.mp4')}\n",
    "metadata_files = {f.stem: f for f in metadata_dir.glob('*.json')}\n",
    "\n",
    "# Find pairs of videos and metadata that have the same filename prefix\n",
    "matched_files = [(video_files[f], metadata_files[f]) for f in video_files if f in metadata_files]\n",
    "\n",
    "print(f\"Matched files: {len(matched_files)}\")\n",
    "if not matched_files:\n",
    "    raise ValueError(\"No matching video-metadata pairs found.\")\n",
    "\n",
    "# Step 2: Shuffle the matched files for randomness\n",
    "random.shuffle(matched_files)\n",
    "\n",
    "# Step 3: Build folder structure for the dataset\n",
    "train_dir = output_dir / 'train'\n",
    "train_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def create_tar_files(pairs, output_dir):\n",
    "    \"\"\"Helper function to create TAR files with batch_size pairs of video/metadata each.\"\"\"\n",
    "    #Note: you should adjust 75 to the number that fits your dataset.\n",
    "    # - You want to avoid having tar files bigger than 20GB (ideally 1-2gb per file)\n",
    "    # - You don't want more than 10K tar files\n",
    "    # more details: https://huggingface.co/docs/hub/repositories-recommendations\n",
    "\n",
    "    \n",
    "    batch_size = 75\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        tar_filename = output_dir / f'{i // batch_size:05d}.tar'\n",
    "        with tarfile.open(tar_filename, 'w') as tar:\n",
    "            shard_metadata = {\"num_examples\": len(pairs[i:i + batch_size])}\n",
    "            for j, (video_file, metadata_file) in enumerate(pairs[i:i + batch_size], start=i):\n",
    "\n",
    "                # Create WebDataset compatible names\n",
    "                base_name = f'{j:06d}'\n",
    "                video_name = f'{base_name}.mp4'\n",
    "                json_name = f'{base_name}.json'\n",
    "                \n",
    "                # Add files to tar with new names\n",
    "                tar.add(video_file, arcname=video_name)\n",
    "                \n",
    "                # Read and modify JSON content\n",
    "                with open(metadata_file, 'r') as f:\n",
    "                    metadata = json.load(f)\n",
    "                metadata['original_video_filename'] = video_file.name\n",
    "                metadata['original_json_filename'] = metadata_file.name\n",
    "                \n",
    "                # Write modified JSON to tar\n",
    "                json_content = json.dumps(metadata, ensure_ascii=False).encode('utf-8')\n",
    "                json_info = tarfile.TarInfo(name=json_name)\n",
    "                json_info.size = len(json_content)\n",
    "                tar.addfile(json_info, fileobj=io.BytesIO(json_content))\n",
    "\n",
    "            # Add shard metadata\n",
    "            shard_metadata_content = json.dumps(shard_metadata).encode('utf-8')\n",
    "            shard_metadata_info = tarfile.TarInfo(name='__meta__.json')\n",
    "            shard_metadata_info.size = len(shard_metadata_content)\n",
    "            tar.addfile(shard_metadata_info, fileobj=io.BytesIO(shard_metadata_content))\n",
    "                \n",
    "        print(f'Created {tar_filename} with {len(pairs[i:i + batch_size])} elements.')\n",
    "\n",
    "# Step 4: Push all video/json pairs into TAR files of 75 elements\n",
    "print(\"Creating TAR files for the dataset...\")\n",
    "create_tar_files(matched_files, train_dir)\n",
    "\n",
    "print(\"TAR file creation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset is ready as a webdataset, we push it to the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from datasets import config\n",
    "\n",
    "# default is 100, given the size of the dataset, probably is better to make this number smaller\n",
    "\n",
    "config.PARQUET_ROW_GROUP_SIZE_FOR_BINARY_DATASETS = 10 \n",
    "def upload_dataset_to_hub(local_dir, repo_id):\n",
    "    # Initialize Hugging Face API\n",
    "    api = HfApi()\n",
    "\n",
    "    # Ensure the repository exists (or create it)\n",
    "    try:\n",
    "        api.create_repo(repo_id=repo_id, repo_type=\"dataset\", exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating/checking repo: {e}\")\n",
    "        return\n",
    "\n",
    "    # Load the dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = load_dataset(\"webdataset\", data_dir=local_dir, split=\"train\")\n",
    "    \n",
    "    # Removing some columns that are not used\n",
    "    columns_to_remove = ['__key__', '__url__']\n",
    "    updated_dataset = dataset.remove_columns(columns_to_remove)\n",
    "    print(\"Dataset loaded. Pushing to Hub...\")\n",
    "\n",
    "\n",
    "    # Push the dataset to the Hub\n",
    "    updated_dataset.push_to_hub(repo_id)\n",
    "\n",
    "    print(f\"Dataset successfully pushed to {repo_id}\")\n",
    "\n",
    "# Usage\n",
    "local_dataset_dir = \"hf_dataset\"\n",
    "hf_repo_id = \"yourrepo/identifier\"\n",
    "\n",
    "upload_dataset_to_hub(local_dataset_dir, hf_repo_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once pushed, you can consume the dataset like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "#full dataset (600GB of data)\n",
    "dataset = load_dataset(\"yourrepo/identifier\", split=\"train\")\n",
    "print(dataset[0]['json']) # Access the metadata and speech to text of the first sample\n",
    "dataset['0']['mp4'] # Access the video\n",
    "\n",
    "#dataset streaming (will only download the data as needed)\n",
    "dataset = load_dataset(\"yourrepo/identifier\", split=\"train\", streaming=True)\n",
    "sample = next(iter(dataset))\n",
    "print(sample['json'])\n",
    "\n",
    "with open('sample.mp4', 'wb') as video_file:\n",
    "    video_file.write(sample['mp4'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
